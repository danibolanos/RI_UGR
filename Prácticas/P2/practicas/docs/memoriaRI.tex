\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish, es-tabla, es-lcroman, es-noquoting]{babel}
\usepackage[left=3cm,top=3.5cm,right=3cm,bottom=3.5cm]{geometry} 
\selectlanguage{spanish}
\usepackage{graphicx} %package to manage images
\graphicspath{ {./fotos/} }
\usepackage{enumerate} % enumerados
\usepackage{color}
\usepackage{listings}
\usepackage{makeidx}
\usepackage{hyperref}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{
\normalfont \normalsize
\horrule{0.5pt} \\[0.4cm]
\huge Práctica 1: \\ Parser de documentos con TIKA. \\
\horrule{2pt} \\[0.5cm]
\begin{figure}[h]
\centering
\includegraphics[scale=0.75]{logoUGR.jpg}
\textsc{\textbf{Recuperación de Información (2020-2021)}} \\ [10pt]
\end{figure}\
}

\author{ Daniel Bolaños Martínez \\ Fernando de la Hoz Moreno \\ Grupo 1 - Martes 11:30h}
\date{}

\lstset{ %
language=Java,                % choose the language of the code
basicstyle=\footnotesize,       % the size of the fonts that are used for the code
numbers=none,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it is 1 each line will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code
backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,           % adds a frame around the code
tabsize=2,          % sets default tabsize to 2 spaces
captionpos=b,           % sets the caption-position to bottom
breaklines=true,        % sets automatic line breaking
breakatwhitespace=false,    % sets if automatic breaks should only happen at whitespace
escapeinside={\%*}{*)}          % if you want to add a comment within your code
}

\begin{document}

\maketitle

\newpage

\tableofcontents

\clearpage

\section{Introducción.}

En esta práctica veremos cómo poder extraer información (texto) a partir de un documento dado, independiente del formato del archivo, paso previo para cualquier proceso de recuperación de información o análisis de textos.\cite{Guion}\\

Se pide realizar un programa capaz de extraer toda la información que cuelga de un directorio que contenga como mínimo 10 ficheros distintos con al menos 3 formatos diferentes y que según la opción, realice las siguientes funciones:

\begin{itemize}
\item \textbf{-d}: Creación de una tabla con nombre, codificación, tipo e idioma de cada fichero.
\item \textbf{-l}: Extracción de todos los enlaces de cada fichero.
\item \textbf{-t}: Generación de un archivo CSV que contenga las ocurrencias de las palabras para cada fichero.
\end{itemize}

Antes de comenzar a analizar y parsear la lista de archivos del directorio debemos, para todas las opciones:

\begin{itemize}
\item Obtener la lista con los nombres de todos los ficheros contenidos en el directorio pasado por parámetro.
\item Definir un objeto \textit{tika} y establecer el límite de caracteres.
\end{itemize}

\begin{lstlisting}
File directorio = new File(args[1]);
String[] archivos = directorio.list();
//Creamos un objeto Tika
Tika tika = new Tika();
//Establecemos el limite de caracteres para los strings
tika.setMaxStringLength(1000000000);
//Representa los metadatos del documento
Metadata metadata = new Metadata();
\end{lstlisting}\

Los documentos que queramos analizar los debemos almacenar en el directorio \textit{docs} dentro de la carpeta \textit{src}.

\section{Opción \textbf{-d}.}

Para esta opción, crearemos una tabla que recopile para cada archivo del directorio, información sobre su nombre, codificación, tipo de documento y lenguaje.\\

Declaramos un manejador de tipo \textit{BodyContentHandler} estableciendo el límite de caracteres y definimos los objetos para los metadatos, contexto y parser (usamos \textit{AutoParser} para crear un \textit{parser} específico según el tipo del fichero). Estos objetos, serán pasados como parámetros a la función \textit{parse} junto con el \textit{FileInputStream} del fichero que estamos analizando.\\ 

A partir de los metadatos de cada archivo, podemos extraer el nombre del fichero (RESOURCE\_NAME\_KEY), la codificación (CONTENT\_ENCODING) y el tipo (CONTENT\_TYPE).\cite{Guion}\\

Finalmente, para identificar el lenguaje declaramos un objeto \textit{LanguageIdentifier} y le pasamos un string con el contenido en texto plano del archivo.\\

\begin{lstlisting}
//Recorremos para cada archivo del directorio
for(String file : archivos){
  File f = new File("./" + args[1] + "/" + file);
  //Establece el limite de caracteres en el constructor
  BodyContentHandler handler = new BodyContentHandler(1000000000);
  //Guarda la informacion del contexto concreto para el ContentHandler
  ParseContext parseContext = new ParseContext();
  //Creamos un objeto para parsear el documento
  AutoDetectParser parser = new AutoDetectParser();
  /*Utilizamos parser para que a traves del stream de datos del documento, content, nos devuelva el contenido en el objeto handler y los metadatos en medatada*/
  FileInputStream stream = new FileInputStream(f);
  try {
    parser.parse(stream, handler, metadata, parseContext);
  } finally{
      stream.close();
  }
  //Obtenemos los metadatos del documento.
  tika.parse(f, metadata);
  /*Creamos un objeto LanguageIdentifier al que le pasamos un string con el contenido del documento para extraer el idioma en el que este escrito.*/
  LanguageIdentifier object = new LanguageIdentifier(handler.toString());
  /*Extraemos el nombre del documento, la codificacion, el MIME tipo del archivo y el idioma en que esta escrito.*/
  String nam = metadata.get(Metadata.RESOURCE_NAME_KEY);
  String encod = metadata.get(Metadata.CONTENT_ENCODING);
  String type = metadata.get(Metadata.CONTENT_TYPE);
  String lang = object.getLanguage();
}
\end{lstlisting}\

Para mostrar por pantalla la tabla, hemos creado un formato con un tabulado personalizado que nos proporciona los resutados de la imagen \ref{tabla}.

\section{Opción \textbf{-l}.}

Para esta opción, queremos extraer todos los enlaces para cada fichero del directorio pasado como parámetro. Para ello iteramos sobre cada fichero del directorio.\\

Declaramos un manejador de tipo \textit{LinkContentHandler} que como hemos visto en los ejemplos, es necesario para obtener los links.\cite{tika}\\

Además, tenemos que declarar los objetos para los metadatos, contexto y parser, que se pasarán a la función \textit{parse} junto con el \textit{FileInputStream} del fichero que estamos analizando.\\

Finalmente guardaremos los links extraídos a partir del objeto \textit{LinkContentHandler} en una lista y para cada archivo los mostraremos en el caso de que no sea vacía.\\

\begin{lstlisting}
//Para cada archivo del directorio
for(String file : archivos){
  File f = new File("./" + args[1] + "/" + file);
  //Definimos un handler para links
  LinkContentHandler linkHandler = new LinkContentHandler();
  //Definimos un objeto para el contexto y el parser
  ParseContext parseContext = new ParseContext();
  AutoDetectParser parser = new AutoDetectParser();
  FileInputStream stream = new FileInputStream(f);
  try {
    parser.parse(stream, linkHandler, metadata, parseContext);
  } finally{
      stream.close();
  }
  //Guardamos los enlaces en una lista de Links
  List<Link> enlaces = linkHandler.getLinks();
  //Imprimimos aquellos enlaces que no sean vacios
  System.out.println("ENLACES del archivo " + file + ":\n");
  if(enlaces.isEmpty())
    System.out.println("NO se han encontrado enlaces.");
  else{
    for(Link l:enlaces)
      System.out.println(l);
}
\end{lstlisting}\

Podemos ver su funcionamiento en la imagen \ref{enlace}.

\clearpage

\begin{figure}[h]
\centering
\includegraphics[scale=0.34]{tabla.png}
\label{tabla}
\caption{Tabla creada con la opción \textbf{-d} para los archivos contenidos en \textit{docs}.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[scale=0.33]{enlaces.png}
\label{enlace}
\caption{Extracto funcionamiento opción \textbf{-l} para 3 documentos.}
\end{figure}

\clearpage

\section{Opción \textbf{-t}.}

Para esta opción, contaremos las ocurrencias de cada palabra para cada archivo del directorio que recorramos. Además, generará un fichero CSV con el siguiente formato: $$palabra ; ocurrencias$$
Donde el número de ocurrencias será descendente.\\

Antes de empezar, crearemos (si no existe) una carpeta dentro del directorio \textit{src} llamada \textit{CSV}, donde generaremos los archivos CSV para cada fichero.\\

Definimos una función externa que introduce las entradas del Map en un stream, lo ordena con los valores del Map y crea otro Map con las entradas ordenadas y con los valores del antiguo.\\
  
\begin{lstlisting}
public static Map<String, Integer> sortByValue(final Map<String, Integer> wordCounts) {
  return wordCounts.entrySet().stream().sorted((Map.Entry.<String, Integer>comparingByValue().reversed())).collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue, (e1, e2) -> e1, LinkedHashMap::new));
}
\end{lstlisting}

\begin{lstlisting}
for(String file : archivos){
  File f = new File("./" + args[1] + "/" + file);
  //Eliminamos la extensión para los nombres de los archivos
  String s;
  String[] splittedName = file.split("\\.");
  String name = splittedName[0];
  //Establece el límite de caracteres en el constructor
  BodyContentHandler handler = new BodyContentHandler(1000000000);
  //Definimos un objeto para el contexto y el parser
  ParseContext parseContext = new ParseContext();
  AutoDetectParser parser = new AutoDetectParser();
  FileInputStream stream = new FileInputStream(f);
  try {
    parser.parse(stream, handler, metadata, parseContext);
  } finally{
      stream.close();
  }
  //Pasamos el contenido del documento del objeto handler a un string
  String str = handler.toString();
  //Crea un Stream a partir del string.
  //Mapea el elemento del stream a un array de strings con map().
  //Los elementos de este array de strings son las palabras separadas a través de split().
  //split() utiliza una expresion regular para separar las palabras.
  //flatMap() mapea el array a un stream que tiene como elementos los elementos del array.
  //collect() nos devuelve una lista de strings con los elementos del stream
  List <String> list = Stream.of(str).map(w -> w.split("\\s+|[\\.\\;\\:\\,\\-]\\s+|[()?!]\\s+|[0-9]")).flatMap(Arrays::stream).collect(Collectors.toList());
  //stream() crea una Stream cuyos elementos son los elementos de list
  //collect() nos devuelve un Map que se crea a través de Collectors.toMap()
  //Collectors.toMap() crea un Map donde los keys son los elementos del stream
  //Cada key tiene como valor 1 en el Map y las colisioines se resuelven sumando los valores
  //Asi obtenemos un Map donde cada Key es la palabra que aparece y el valor su frecuencia.
  Map <String, Integer > wordCounter = list.stream().collect(Collectors.toMap(w -> w.toLowerCase(), w -> 1, Integer::sum));
  //Ordenamos el Map segun los valores.
  wordCounter = sortByValue(wordCounter);
  Iterator it = wordCounter.keySet().iterator();
  //Creamos archivos .csv con el recuento de las palabras de cada documento en CSV/
  PrintWriter writer = new PrintWriter("./CSV/" + name + ".csv");
  writer.print("Text" + ";" + "Size\n");
  while(it.hasNext()){
    String key = (String) it.next();
    writer.print(key + ";" + wordCounter.get(key) + "\n");
  }
  writer.close();
}
\end{lstlisting}

\begin{lstlisting}
Text;Size
de;62
en;23
;19
¿qué;17
\end{lstlisting}


\section{Nube de palabras.}

Podemos generar nubes de palabras a partir de los ficheros CSV obtenidos con la opción \textbf{-t} desde la web de \textit{wordart} \cite{Nube}.\\

Hemos elegido dos de entre todos los ficheros CSV generados y a continuación mostraremos el resultado. Además, hemos configurado una forma y color diferente para cada nube generada.

\clearpage

\begin{figure}[h]
\centering
\includegraphics[scale=0.61]{nube1.png}
\includegraphics[scale=0.61]{nube2.png}
\label{nubes}
\caption{Nubes de \textit{Presentacion\_Daniel\_Fernando.csv} y \textit{presentacion\_p0.csv}}
\end{figure}
\clearpage

\section{Método de compilación.}

Ejecutamos las siguientes órdenes a través de terminal situados en el directorio \textit{src}.

\begin{lstlisting}
> javac -cp ./tika-app-1.23.jar P1Tika.java
> java -cp ./tika-app-1.23.jar:. P1Tika -option docs
\end{lstlisting}

Donde \textit{-option} puede ser una de las opciones de la siguiente lista $[-d,-l,-t]$.

\section{Trabajo en Grupo.}

El trabajo lo hemos repartido, en primera instancia, de la siguiente manera:

\begin{itemize}
\item \textbf{Daniel Bolaños Martínez}: Opción -d (formato de tabla) y  Opción -l.
\item \textbf{Fernando de la Hoz Moreno}: Opción -d y Opción -t.
\end{itemize}

No obstante, hemos mantenido el contacto durante el desarrollo de la práctica y hemos colaborado conjuntamente en la elaboración del proyecto completo.

\patchcmd{\thebibliography}{\section*}{\section}{}{}
\begin{thebibliography}{}
\bibitem{Guion} Guión de la práctica 1 de la asignatura.
\bibitem{tika} \url{https://tika.apache.org/1.24.1/examples.html}
\bibitem{Nube} \url{https://wordart.com/create}
\end{thebibliography}

\end{document}